{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d079b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.8.10\n",
    "# pytorch: 1.10.1+cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5ee159a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build char-word based NER Task...\n",
      "build gaz embedding...\n",
      "Build the Gaz bilstm...\n",
      "build batched crf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./relation-extraction/pretrained_models/bert-base-chinese were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import itertools\n",
    "import ner_A\n",
    "import ner_filter\n",
    "import rel_ext_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb8dbf3",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "233121a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./annual_report_23.json') as r:\n",
    "    corpus = json.loads(r.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de5852",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48badb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only chinese documents\n",
    "corpus = [stock for stock in corpus if stock['language']=='zh']\n",
    "\n",
    "# adapt to training data\n",
    "r1 = r'(?<=[a-zA-Z.]{1})(?=[\\u4e00-\\u9fff]{1})'\n",
    "r3 = r'（\\d+歲）'\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    for j in range(len(corpus[i]['text'])):\n",
    "        x = corpus[i]['text'][j]['text']\n",
    "        x = re.sub(r3, '，', x) #age in this format is stranger to the model\n",
    "        x = re.sub('[－、•╱]', '，', x) #uncommon punctuation in training data\n",
    "        x = re.sub('[（） ]', '', x) #uncommon punctuation in training data\n",
    "        x = re.sub(r1, '，', x) #add comma after english and before chinese\n",
    "        x = re.sub('[\\uf098\\uf099]', '，', x) #unwanted characters \n",
    "        corpus[i]['text'][j]['text'] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd4156f",
   "metadata": {},
   "source": [
    "### NER labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c9c23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raymond/git/ctil/./Chinese-NER/model/cw_ner/model/crf.py:161: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1273.)\n",
      "  cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER counts {'NAME': 1063, 'ORG': 2842}\n"
     ]
    }
   ],
   "source": [
    "texts, instances = ner_A.get_compatible_input(corpus)\n",
    "ner_predictions = ner_A.predict(instances)\n",
    "labelled_corpus = ner_A.get_ner_labelled_corpus(corpus, texts, ner_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519fa73",
   "metadata": {},
   "source": [
    "### Filter unwanted NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ee97876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER counts {'NAME': 549, 'ORG': 1870}\n",
      "# Unique Valid person/org: 397/1389\n",
      "# Unique Invalid person/org: 219/389\n"
     ]
    }
   ],
   "source": [
    "labelled_corpus, valid_org, valid_person, invalid_org, invalid_person =\\\n",
    "    ner_filter.filter_ner(labelled_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "681bece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inspecting (in)valid person/org lists.\n",
    "# import numpy as np\n",
    "# def get_count(ls):\n",
    "#     ls = np.array(ls)\n",
    "#     return sorted([((ls==x).sum(), x) for x in np.unique(ls)], reverse=True)\n",
    "\n",
    "# get_count(valid_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd9dfc7",
   "metadata": {},
   "source": [
    "### Relation extraction (Person to Person or Person to Org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a42c2b0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give up stripping\n",
      "The size of tensor a (540) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Give up stripping\n",
      "The size of tensor a (540) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Give up stripping\n",
      "The size of tensor a (540) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Give up stripping\n",
      "The size of tensor a (526) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Give up stripping\n",
      "Give up stripping\n",
      "The size of tensor a (556) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Give up stripping\n",
      "The size of tensor a (556) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "valid_relation = []\n",
    "invalid_relation = []\n",
    "\n",
    "for stock in labelled_corpus:\n",
    "    for doc in stock['text']:\n",
    "        doc['relation_list'] = []\n",
    "        for h, t in itertools.combinations(doc['ner'], 2):\n",
    "            \n",
    "            is_per_per = (h['label_'] == 'NAME' and t['label_'] == 'NAME')\n",
    "            is_per_org = (h['label_'] == 'NAME' and t['label_'] == 'ORG') or\\\n",
    "                         (h['label_'] == 'ORG' and t['label_'] == 'NAME')\n",
    "            \n",
    "            # Only person-to-person and person-to-organization\n",
    "            if not is_per_per and not is_per_org:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                item = rel_ext_A.gen_item(doc, h, t, max_length=500)\n",
    "                model_input = rel_ext_A.tokenize_item(item)\n",
    "                logits = rel_ext_A.predict(model_input)\n",
    "                probas = rel_ext_A.get_proba(logits, mode='GROUPED_SUM')\n",
    "#                 print(h['text'], t['text'], probas[0:2])\n",
    "\n",
    "                relation = dict(\n",
    "                    predicate=probas[0][1],\n",
    "                    subject_type=h['label_'],\n",
    "                    object_type=t['label_'],\n",
    "                    subject=h['text'],\n",
    "                    object=t['text'],\n",
    "                )\n",
    "    \n",
    "                # Only keep meaningful relations\n",
    "                if is_per_per and relation['predicate'] != 'Unknown' or\\\n",
    "                    is_per_org and relation['predicate'] == 'Work':\n",
    "                    doc['relation_list'].append(relation)\n",
    "                    valid_relation.append(relation)\n",
    "                \n",
    "                else:\n",
    "                    invalid_relation.append(relation)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Exception can happen when the text length is too long.\n",
    "                print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ece1ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final_corpus.json', 'w') as f:\n",
    "    f.write(json.dumps(labelled_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2944d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Identified', len(valid_relation), 'relations')\n",
    "# print('Dropped', len(invalid_relation), 'relations')\n",
    "# for relation in sorted(valid_relation, key=lambda x: x['predicate'])[:10]:\n",
    "#     print(relation['subject'], '>', relation['predicate'], '>', relation['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f39d2c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['三六零安全科技股份有限公司上交所',\n",
       " '三號幹線郊野公園段有限公司',\n",
       " '上海吉祥航空股份有限公司上海證券交易所',\n",
       " '上海同濟科技實業股份有限公司',\n",
       " '上海國際港務集團股份有限公司',\n",
       " '上海大眾公用事業集團股份有限公司',\n",
       " '上海復旦張江生物醫藥股份有限公司',\n",
       " '上海振華重工集團股份有限公司',\n",
       " '上海時代航運有限公司',\n",
       " '上海東方明珠新媒體股份有限公司上海證']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Listing valid chinese organization names.\n",
    "display(list(filter(lambda x: not re.match(r'^[a-zA-Z.]+$', x), sorted(set(valid_org))))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf5bad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['丁良輝', '中國領', '于正人', '井賢棟', '付丹偉', '代者', '伍成業', '何平何平', '何成效', '何漢明']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Listing valid chinese organization names.\n",
    "display(list(filter(lambda x: not re.match(r'^[a-zA-Z.]+$', x), sorted(set(valid_person))))[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
